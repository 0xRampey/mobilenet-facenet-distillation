{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose and split your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prudhvi/Desktop/Distillation/mobilenet-facenet-distillation/Facenet_impl.ipynb:61: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  \"### Results of current model being used\\n\",\n"
     ]
    }
   ],
   "source": [
    "import ipynb.fs #Boilerplate for ipynb notebooks as modules\n",
    "\n",
    "from .defs.Facenet_impl import get_face_embeddings \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from facenet.src import facenet\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your seed\n",
    "seed = 1\n",
    "np.random.seed(seed=seed)\n",
    "#Obtain dataset\n",
    "dataset_path = \"~/Desktop/Datasets/lfw/lfw_mtcnnpy_160\"\n",
    "dataset = facenet.get_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, min_nrof_images_per_class, nrof_train_images_per_class):\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for cls in dataset:\n",
    "        paths = cls.image_paths\n",
    "        # Remove classes with less than min_nrof_images_per_class\n",
    "        if len(paths)>=min_nrof_images_per_class:\n",
    "            np.random.shuffle(paths)\n",
    "            train_set.append(facenet.ImageClass(cls.name, paths[:nrof_train_images_per_class]))\n",
    "            test_set.append(facenet.ImageClass(cls.name, paths[nrof_train_images_per_class:]))\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 5749\n",
      "Number of images: 12020\n",
      "Number of classes: 5749\n",
      "Number of images: 1213\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = split_dataset(dataset, 0, 35)\n",
    "train_paths, train_labels = facenet.get_image_paths_and_labels(train_set)\n",
    "print('Number of classes: %d' % len(train_set))\n",
    "print('Number of images: %d' % len(train_paths))\n",
    "test_paths, test_labels = facenet.get_image_paths_and_labels(test_set)\n",
    "print('Number of classes: %d' % len(test_set))\n",
    "print('Number of images: %d' % len(test_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get face embeddings for train/test datasets and save them to disk while doing this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your data to disk after splitting\n",
    "train_set_emb = get_face_embeddings(train_set, save_to_disk = True, save_dir = \"/home/prudhvi/Desktop/Datasets/lfw/lfw_mtcnn_160_split/train/data/\")\n",
    "test_set_emb = get_face_embeddings(test_set, save_to_disk = True, save_dir = \"/home/prudhvi/Desktop/Datasets/lfw/lfw_mtcnn_160_split/test/data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_dict(paths, emb_array):\n",
    "    emb_dict={}\n",
    "    for i in range(len(paths)):\n",
    "        image_name = paths[i].split('/')[-1]\n",
    "        emb_dict[image_name] = emb_array[i]\n",
    "    return emb_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of the face embeddings and their respective images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_dict = create_emb_dict(train_paths, train_set_emb)\n",
    "test_set_dict = create_emb_dict(test_paths, test_set_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionaries to disk\n",
    "np.save('data/face_emb_logits/train_set',train_set_dict)\n",
    "np.save('data/face_emb_logits/test_set',test_set_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
